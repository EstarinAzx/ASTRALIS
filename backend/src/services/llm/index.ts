// ============================================================================
// LLM Service - Main Entry Point
// Parser-First Architecture: Pattern parser generates ‚Üí LLM enhances
// ============================================================================

import type { VerbosityMode, AnalysisResult } from './types.js';
import { parseCode } from './parser.js';

// Re-export types
export type {
    VerbosityMode,
    AnalysisResult,
    FlowNode,
    FlowEdge,
    NodeShape,
    SectionColor
} from './types.js';

/**
 * Build the enhancement prompt for LLM review
 */
function buildEnhancerPrompt(): string {
    return `You are a CODE FLOWCHART ENHANCER. You receive a flowchart generated by a parser and improve it.

YOUR ROLE:
1. KEEP all the accurate line numbers from the parser (these are ground truth)
2. IMPROVE node labels to be more human-readable and semantic
3. ADD any missing decision branches (YES/NO paths) if the parser missed them
4. FIX any incorrect node connections or edge labels
5. ENHANCE narratives to explain what the code actually does

RULES:
- DO NOT change lineStart or lineEnd values (parser got these right)
- DO NOT remove nodes unless they are duplicates
- DO NOT add nodes for code that doesn't exist
- IMPROVE labels: "Function: handleSubmit" ‚Üí "Handle Form Submission"
- IMPROVE subtitles where appropriate
- ENSURE decision nodes (diamonds) have YES and NO edges

OUTPUT: Return the enhanced flowchart as valid JSON in the exact same format.
Only return JSON, no explanations.`;
}

/**
 * Main LLM call function - generates flowchart from code
 * Uses Parser-First architecture: Pattern parser generates, LLM enhances
 */
export async function callLLM(
    code: string,
    fileName: string,
    language: string,
    mode: VerbosityMode = 'standard'
): Promise<AnalysisResult> {
    console.log(`üöÄ ASTRALIS: Processing ${fileName} (${language}, mode: ${mode})`);
    console.log(`üìù Code length: ${code.length} chars, ${code.split('\n').length} lines`);

    // Step 1: Parser generates baseline (always runs first)
    console.log('üìä Step 1: Pattern parser generating baseline...');
    const parserResult = parseCode(code, fileName, language);
    console.log(`‚úÖ Parser generated ${parserResult.nodes.length} nodes, ${parserResult.edges.length} edges`);

    // Step 2: LLM enhances (if API key available)
    const apiKey = process.env.LLM_API_KEY;
    const apiUrl = process.env.LLM_API_URL || 'https://openrouter.ai/api/v1/chat/completions';
    const model = process.env.LLM_MODEL || 'nvidia/nemotron-3-nano-30b-a3b:free';

    if (!apiKey) {
        console.log('‚ö†Ô∏è No LLM API key, returning parser result as-is');
        return parserResult;
    }

    try {
        console.log('ü§ñ Step 2: LLM enhancing flowchart...');

        const response = await fetch(apiUrl, {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
                'HTTP-Referer': 'http://localhost:3001',
                'X-Title': 'ASTRALIS Flowchart Visualizer',
            },
            body: JSON.stringify({
                model,
                messages: [
                    { role: 'system', content: buildEnhancerPrompt() },
                    {
                        role: 'user',
                        content: `SOURCE CODE (${language}):\n\`\`\`${language}\n${code}\n\`\`\`\n\nPARSER-GENERATED FLOWCHART:\n${JSON.stringify(parserResult, null, 2)}\n\nPlease enhance this flowchart with better labels and fix any issues.`
                    },
                ],
                temperature: 0.2, // Low temp for consistency
                max_tokens: 8000,
            }),
        });

        if (!response.ok) {
            console.error(`‚ùå LLM API error: ${response.status} ${response.statusText}`);
            return parserResult; // Fallback to parser result
        }

        const data = await response.json() as { choices?: Array<{ message?: { content?: string } }> };
        let content = data.choices?.[0]?.message?.content?.trim();

        if (!content) {
            console.log('‚ö†Ô∏è LLM returned empty, keeping parser result');
            return parserResult;
        }

        // Extract JSON from response
        if (content.includes('```json')) {
            content = content.replace(/```json\s*/g, '').replace(/```/g, '').trim();
        } else if (content.includes('```')) {
            content = content.replace(/```\s*/g, '').trim();
        }

        if (content.includes('{')) {
            const jsonStart = content.indexOf('{');
            const jsonEnd = content.lastIndexOf('}');
            if (jsonStart !== -1 && jsonEnd !== -1) {
                content = content.substring(jsonStart, jsonEnd + 1);
            }
        }

        const enhanced = JSON.parse(content) as AnalysisResult;

        // Validate the enhanced result
        if (!enhanced.nodes || enhanced.nodes.length === 0) {
            console.log('‚ö†Ô∏è LLM returned invalid result, keeping parser result');
            return parserResult;
        }

        console.log(`‚úÖ LLM enhanced: ${parserResult.nodes.length} ‚Üí ${enhanced.nodes.length} nodes`);

        // Preserve parser's accurate line numbers if LLM messed them up
        enhanced.nodes.forEach((node, i) => {
            const parserNode = parserResult.nodes[i];
            if (parserNode && (!node.lineStart || !node.lineEnd)) {
                node.lineStart = parserNode.lineStart;
                node.lineEnd = parserNode.lineEnd;
            }
        });

        return enhanced;

    } catch (error) {
        console.error('‚ùå LLM enhancement error:', error);
        return parserResult; // Fallback to parser result
    }
}
